#!/usr/bin/env python3
"""
Send transcripts to OpenAI GPT to generate clip summaries per category instructions.
Refactored to use centralized UPLOADS_DIR from config.py and environment for API key.
"""
import os
import sys
import json
from pathlib import Path

from config import UPLOADS_DIR
from openai import OpenAI

# ‚îÄ‚îÄ‚îÄ Load API key ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
API_KEY = os.getenv("OPENAI_API_KEY")
if not API_KEY:
    print("‚ùå Missing OPENAI_API_KEY environment variable", file=sys.stderr)
    sys.exit(1)
client = OpenAI(api_key=API_KEY)

# ‚îÄ‚îÄ‚îÄ Configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
PROMPT_INSTRUCTIONS = {
    "Oracle_Chronicles": (
        """You‚Äôre editing a premium boxing education series called Oracle Chronicles.
The video features padwork, sparring, and bagwork sessions with real-time coach commentary.
From the following timestamped transcript, **condense** to just the single most valuable moments‚Äî
each on its own line in the format `start-end: spoken text`.
Aim for ~52‚Äâs per moment, adjust boundaries as needed to capture full combo + reaction.
Do NOT output any JSON or extra formatting‚Äîjust lines like:
275.0-327.0: Nice and sharp combination, keep it tight and quick."""
    ),
    "Training": (
        """You‚Äôre editing boxing training footage (bag work, pad work, sparring) without commentary.
If the transcript explicitly says \"clip the last three minutes,\" do exactly that.
Otherwise:
 ‚Ä¢ If there‚Äôs any spoken dialogue, clip the entire spoken section plus ~5‚Äì10‚Äâs before and after for context.
 ‚Ä¢ If there‚Äôs no dialogue, estimate the most action-packed moments based on typical 3-minute rounds.
 ‚Ä¢ Whenever you see audible cues (grunts, bag thuds, crowd noise), include ~10‚Äì20‚Äâs around them.
Aim for generous clips of ~30‚Äì45‚Äâs each, free of repetition.
Output each on its own line in the format `start-end: text or description of action`.
Do NOT output JSON or any extra formatting."""
    ),
    "YouTube_Clipper": (
        """You‚Äôre editing a YouTube boxing tutorial into ~60‚Äâs social clips.
From the timestamped transcript, choose only the clearest standalone lessons.
If a lesson exceeds 60‚Äâs, split into ‚ÄúPart 1: Label,‚Äù ‚ÄúPart 2: Label,‚Äù etc., but avoid repeats.
Output each on its own line `start-end: text`‚Äîno JSON, no extra formatting."""
    ),
    "YouTube_Editor": (
        """You‚Äôre editing uncut boxing instruction for final YouTube.
Extract only polished segments (hooks, intros, demos, CTAs), omit filler/mistakes, obey any ‚Äúdo not include‚Ä¶‚Äù directions.
Aim for 30‚Äì60‚Äâs per clip.
Output each on its own line `start-end: text`‚Äîno JSON, no extra formatting."""
    ),
    "Non-Boxing": (
        """You‚Äôre editing a non-boxing video (business talks, casual banter, interviews, sketches, etc.).  
From the timestamped transcript, identify the single most compelling moments‚Äîinsightful insights, punchlines, key takeaways, or standout reactions.  
 ‚Ä¢ For spoken sections, clip the heart of each idea or joke, including ~3‚Äì5 s of lead-in and follow-through for context.  
 ‚Ä¢ For purely visual moments (pauses, reactions, actions), use the transcript cues to estimate ~5‚Äì10 s highlights.  
 ‚Ä¢ Aim for clips of ~45‚Äì60 s each.  
Output each clip on its own line in the format `start-end: description or spoken text`.  
Do NOT output JSON or any extra formatting‚Äîjust plain lines like:  
275.0-325.0: ‚ÄúWhen you pitch, focus on the problem before the solution‚Äîhere‚Äôs why‚Ä¶‚Äù"""
    ),
}

# ‚îÄ‚îÄ‚îÄ Helper: chunk the transcript to avoid token limits ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def chunks(lines, max_chars=12000):
    chunk, count = [], 0
    for line in lines:
        count += len(line)
        chunk.append(line)
        if count > max_chars:
            yield chunk
            chunk, count = [], 0
    if chunk:
        yield chunk


def main():
    if not UPLOADS_DIR.exists():
        print(f"‚ùå Uploads directory not found: {UPLOADS_DIR}", file=sys.stderr)
        sys.exit(1)

    # Iterate categories
    for category_dir in UPLOADS_DIR.iterdir():
        if not category_dir.is_dir():
            continue
        instr = PROMPT_INSTRUCTIONS.get(category_dir.name)
        if not instr:
            continue

        # Process each transcript JSON file
        for json_file in sorted(category_dir.glob('*.json')):
            # load segments
            data = json.loads(json_file.read_text(encoding='utf-8'))
            if not data:
                continue

            # build lines: "start-end: text"
            lines = [f"{seg['start']}-{seg['end']}: {seg['text']}" for seg in data]
            summary_lines = []
            header = instr + "\n\nTimestamped transcript:\n"

            # Send in manageable chunks
            for chunk in chunks(lines):
                payload = header + "\n".join(chunk)
                print(f"ü§ñ Sending {len(chunk)} lines to GPT for '{json_file.name}'...")
                resp = client.chat.completions.create(
                    model="gpt-4",
                    messages=[{"role": "user", "content": payload}]
                )
                content = resp.choices[0].message.content.strip().splitlines()
                summary_lines.extend(content)

            # Dedupe & sort by start time
            seen, unique = set(), []
            def parse_start(line):
                try:
                    return float(line.split(':',1)[0].split('-',1)[0])
                except:
                    return 0.0

            for line in summary_lines:
                if line and line not in seen:
                    seen.add(line)
                    unique.append(line)
            unique.sort(key=parse_start)

            # Save results
            out_file = json_file.with_suffix('.summary.txt')
            out_file.write_text("\n".join(unique), encoding='utf-8')
            print(f"üíæ Saved condensed summary to {out_file}")

if __name__ == '__main__':
    main()
